{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1504a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os\n",
    "sys.path.insert(0, '')\n",
    "import outputkggeneration as okgg\n",
    "import searchandinvoke as sandi\n",
    "import inputkggeneration as ikgg\n",
    "#persistant the best_model in the disc but described as triple knowledge \n",
    "from rdflib import Graph, Literal, RDF, URIRef\n",
    "# rdflib knows about quite a few popular namespaces, like W3C ontologies, schema.org etc.\n",
    "from rdflib.namespace import FOAF , XSD\n",
    "from rdflib.namespace import NamespaceManager\n",
    "from rdflib import BNode\n",
    "from uuid import uuid4\n",
    "#from inspect import getmembers, isfunction\n",
    "#print(getmembers(sandi, isfunction))\n",
    "import pickle\n",
    "import joblib\n",
    "from joblib import dump,load\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde1db5",
   "metadata": {},
   "source": [
    "# task implemenation plan\n",
    "1. createTask function: taskName, generate a taskId, input datatype, output datatype, aim description\n",
    "\n",
    "    1.1 create a knowledge space (RDF) with initialing a graph to set name and id triples\n",
    "    \n",
    "    1.2 based on pipeline knowledge or input and output parameters semantic matching to select next microservice\n",
    "    \n",
    "    1.3 invoke the service to get output\n",
    "    \n",
    "    1.4 based on the output to generation knowledge into knolwedge space - knowledge generation function\n",
    "    \n",
    "    1.5 repeat 1.2 until final output request satisfied.\n",
    "    \n",
    "    \n",
    "2. knowledge generatie fucntion\n",
    "\n",
    "# pipeline knoweldge generation py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff4d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_memery={}\n",
    "servicelist=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac7ca890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OperatingTask (task_name,input_file_path,desir_output_type,task_domain,namespace,purpose):\n",
    "    output_memery={}\n",
    "    servicelist=[]\n",
    "    st, ctx = knowledge_reasoning (input_file_path,desir_output_type,task_domain,namespace,purpose)\n",
    "    if st == 0:\n",
    "        return st, ctx\n",
    "    g = Graph()\n",
    "    g.parse(\"KGLayer/contextkg.n3\")\n",
    "    openflag = 0\n",
    "    if len(task_name) > 0 and len(desir_output_type)> 0:\n",
    "        if (namespace==''):\n",
    "            namespace='http://aimicroservice.derby.ac.uk'\n",
    "        task_id = str(uuid4())\n",
    "        task= URIRef(namespace+'/'+task_id)\n",
    "        \n",
    "        #verb:\n",
    "        _type = URIRef(namespace+'/task')\n",
    "        has_input = URIRef(namespace+'/input')\n",
    "        has_output = URIRef(namespace+'/desire_output')\n",
    "        has_ioct = URIRef(namespace+'/iocategory')\n",
    "        has_iodt = URIRef(namespace+'/iodatatype')\n",
    "        has_ioshape = URIRef(namespace+'/ioshape')\n",
    "        has_domain = URIRef(namespace+'/domain')\n",
    "        #object:\n",
    "        g.add((task, RDF.type, _type))\n",
    "        if (len(task_domain)>0): \n",
    "            g.add((task, has_domain, Literal(task_domain,lang=\"en\")))     \n",
    "        if len(input_file_path) > 0: \n",
    "            _input = BNode()\n",
    "            input_type = input_file_path.split('.')[1]\n",
    "            g.add((task, has_input, _input))\n",
    "            g.add((_input, has_iodt, URIRef(namespace+'/'+input_type)))        \n",
    "            _output = BNode()\n",
    "            if input_type == 'csv':\n",
    "                openflag=1\n",
    "        g.add((task, has_output, _output))\n",
    "        for x in desir_output_type: \n",
    "            #print(len(x.split('.')))\n",
    "            if len(x.split('.'))==1:\n",
    "                #print(x)\n",
    "                g.add((_output, has_iodt, URIRef(namespace+'/'+x)))\n",
    "            if len(x.split('.'))>1:\n",
    "                g.add((_output, has_ioct, URIRef(namespace+'/'+x.split('.')[0])))\n",
    "                g.add((_output, has_iodt, URIRef(namespace+'/'+x.split('.')[1])))\n",
    "            if len(x.split('.'))>2:\n",
    "                g.add((_output, has_ioshape, Literal(x.split('.')[2],lang=\"en\")))\n",
    "        print(task_id+' task_input KG is generated')\n",
    "        g.serialize(destination='KGLayer/contextkg'+\".n3\")\n",
    "        g.close()\n",
    "        ctx=''\n",
    "        if openflag ==1:\n",
    "            df = pd.read_csv(input_file_path)\n",
    "            ctx = ikgg.task_panda_inputKG(namespace, task_id, task_domain, purpose, df, input_file_path, desir_output_type)\n",
    "    \n",
    "        output_value=composition_flow(input_file_path,desir_output_type,input_file_path, task_id)\n",
    "        servicelist.append(output_value[1])\n",
    "        if type(output_value[0]) is int:\n",
    "            print ('no solution find')\n",
    "        else:\n",
    "            okgg.task_output_workflowKG (servicelist,task_id,namespace)\n",
    "            okgg.savemodel(task_id,output_value[0],namespace,ctx,servicelist)\n",
    "            if openflag == 1:\n",
    "                okgg.outputlearning(output_value[2]['datafile.pandas'],task_id,output_value[0], namespace)    \n",
    "        return output_value\n",
    "    else:\n",
    "        return 'erro: nothing is generated'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984e0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composition_flow(input_path,desir_output,input_data, task_id):\n",
    "    #servicelist = []\n",
    "    \n",
    "    print('############Composition start##############')\n",
    "    output_value = sandi.searchandinvoke ('',[input_path.split('.')[1]], desir_output, [input_path], task_id, servicelist, output_memery)\n",
    "    #print('out while step: ',output_value[0])\n",
    "    if type(output_value[0]) is int:\n",
    "        if output_value[0]==505:\n",
    "            servicelist.append(output_value[1])    \n",
    "        input_type_test = [input_path.split('.')[1]]\n",
    "        input_value = [input_path]\n",
    "        #type(i) is int\n",
    "        controlgate=0\n",
    "        while(type(output_value[0]) is int):\n",
    "            print('------------In while-----------')\n",
    "            output_value = sandi.searchandinvoke ('',input_type_test, '', input_value, task_id, servicelist,output_memery)\n",
    "            #print(output_value[0])\n",
    "            if type(output_value[0]) is int:\n",
    "                if (output_value[0]==505 and controlgate<5):\n",
    "                    servicelist.append(output_value[1])\n",
    "                    controlgate=controlgate+1\n",
    "                else:\n",
    "                    print('---end while and composition fails----')\n",
    "                    break\n",
    "            else:\n",
    "                if output_value[1] not in servicelist:\n",
    "                    servicelist.append(output_value[1])\n",
    "                    flag = output_match_nextms(output_value[1])\n",
    "                    if flag!=0:\n",
    "                        input_type_test = []\n",
    "                        input_value = [output_value[0]]\n",
    "                        #print(input_value)\n",
    "                        for xf in flag:\n",
    "                            #print('I know here is a problem')\n",
    "                            input_type_test.append(xf[1]) \n",
    "                            #print('check input_value: ',len(input_value[0]))\n",
    "                        output_value = sandi.searchandinvoke ('',input_type_test, desir_output, input_value, task_id, servicelist, output_memery)\n",
    "                        if (len(output_value)<3):\n",
    "                            print('...')\n",
    "                        else:\n",
    "                            output_memery.update(output_value[2])\n",
    "                else:\n",
    "                    print('--------')\n",
    "                \n",
    "        print('-------end while-------')\n",
    "    else:\n",
    "        servicelist.append(output_value[1])\n",
    "        output_memery.update(output_value[2])\n",
    "        print('******** A single service find to complete the task')\n",
    "    if type(output_value[0]) is int:\n",
    "        print('############Composition end unsucessfully##############')\n",
    "    else:\n",
    "        #output_memery.update(output_value)\n",
    "        print('############Composition sucessfully complete##############')\n",
    "    return output_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f658f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_match_nextms(ms_name):\n",
    "    g = Graph()\n",
    "    g.parse(\"registry.n3\")\n",
    "    namespace='http://aimicroservice.derby.ac.uk'\n",
    "    MService = URIRef(namespace+'/'+ms_name)\n",
    "    flag=[]\n",
    "    q = \"\"\"\n",
    "        PREFIX ns1: <http://aimicroservice.derby.ac.uk/>\n",
    "        \n",
    "        SELECT *\n",
    "        WHERE {\"\"\" \n",
    "    if len(ms_name)>0:\n",
    "        q=q+'ns1:'+ms_name+\"\"\" ns1:output ?out . ?out ns1:paramter ?up . {?up ns1:pid ?uid . ?up ns1:iocategory ?c . ?up ns1:iodatatype ?d .} UNION {?up ns1:pid ?uid . ?up ns1:iodatatype ?d}}\"\"\" \n",
    "        #print (q)\n",
    "        qr = g.query(q)\n",
    "        flag_memery=[]\n",
    "        if len(qr)==0:\n",
    "            print('No output find')\n",
    "        else:\n",
    "            for r in qr:\n",
    "                if r[\"c\"] is None:\n",
    "                    if r[\"d\"] is None:\n",
    "                        print ('no output find')\n",
    "                    else:\n",
    "                        #print(r[\"d\"].split('/')[-1])\n",
    "                        if r[\"d\"].split('/')[-1] not in flag_memery:\n",
    "                            flag.append([r[\"uid\"],r[\"d\"].split('/')[-1]])\n",
    "                            flag_memery.append(r[\"d\"].split('/')[-1])\n",
    "                else:\n",
    "                    #print(r[\"c\"].split('/')[-1])\n",
    "                    cate=r[\"c\"].split('/')[-1]\n",
    "                    if r[\"d\"] is None:\n",
    "                        #print ('no specific output')\n",
    "                        if cate not in flag_memery:\n",
    "                            flag.append([r[\"uid\"],cate])\n",
    "                            flag_memery.append(cate)\n",
    "                    else:\n",
    "                        #print(r[\"d\"].split('/')[-1])\n",
    "                        if r[\"d\"].split('/')[-1] not in flag_memery:\n",
    "                            flag.append([r[\"uid\"],cate+'.'+r[\"d\"].split('/')[-1]])\n",
    "                            flag_memery.append(r[\"d\"].split('/')[-1])\n",
    "        #print(flag)\n",
    "        return flag\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3c999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_reasoning (input_file_path,desir_output_type,task_domain,namespace,purpose):\n",
    "    #Matching all paratmeters from context KG return 0 or 100\n",
    "    #if not 100, Matching input [0, 50], Matching ouput [0, 20], matching task_domain [0,10], matching purpose [0,10],\n",
    "    #matching input file type [0,10]\n",
    "        #if >80: provide the exist solution\n",
    "        #if >60 and output not match: provide the workflow as solution without last service\n",
    "        #if >40 to select first service in the workflow to start re-configuration\n",
    "    #else: provide the exist solution\n",
    "    print('Searching for solution ...')\n",
    "    scores = 0\n",
    "    c1 = context_inputsemantic_match(input_file_path,namespace)\n",
    "    c2 = context_outputemantic_match(desir_output_type,namespace)\n",
    "    c3 = context_domain_match(task_domain,namespace)\n",
    "    c4 = context_purpose_match(purpose,namespace)\n",
    "    mscore = {}\n",
    "    if len(c1)> 0:\n",
    "        for c1x in c1:\n",
    "            scores = c1x[1]\n",
    "            ctx = c1x[0]\n",
    "            for c2x in c2:\n",
    "                if c2x[0] == ctx:\n",
    "                    scores = scores + c2x[1]\n",
    "            for c3x in c3:\n",
    "                if c3x[0] == ctx:\n",
    "                    scores = scores + c3x[1]\n",
    "            for c4x in c4:\n",
    "                if c4x[0] == ctx:\n",
    "                    scores = scores + c4x[1]\n",
    "            mscore[ctx] = scores\n",
    "    else:\n",
    "        if len(c2)> 0 :\n",
    "            for c2x in c2:\n",
    "                scores = c2x[1]\n",
    "                ctx = c2x[0]\n",
    "                for c3x in c3:\n",
    "                    if c3x[0] == ctx:\n",
    "                        scores = scores + c3x[1]\n",
    "                for c4x in c4:\n",
    "                    if c4x[0] == ctx:\n",
    "                        scores = scores + c4x[1]\n",
    "                mscore[ctx] = scores\n",
    "        else:\n",
    "            if len (c3>0):\n",
    "                for c3x in c3:\n",
    "                    scores = c3x[1]\n",
    "                    ctx = c3x[0]\n",
    "                    for c4x in c4:\n",
    "                        if c4x[0] == ctx:\n",
    "                            scores = scores + c4x[1]\n",
    "                    mscore[ctx] = scores\n",
    "            else:\n",
    "                for c4x in c4:\n",
    "                    scores = c4x[1]\n",
    "                    ctx = c4x[0]\n",
    "                    mscore[ctx] = scores\n",
    "    mscore = dict(sorted(mscore.items(), key=lambda item: item[1]))\n",
    "    print (mscore)\n",
    "    _key = list(mscore)[-1]\n",
    "    kscores = mscore[_key]\n",
    "    if kscores == 0:\n",
    "        print('No kwnoeldge at all, start to learn knowledge to expore the solution')\n",
    "        return 0, _key\n",
    "    if kscores >=86:\n",
    "        print('from the policy knowledge, solution context found', _key)\n",
    "        return 1, _key\n",
    "    if kscores >=65:\n",
    "        print('Policy knowledge suggests a possilbe workflow to start with')\n",
    "        return 2, _key\n",
    "    if kscores >= 45:\n",
    "        print('Policy knowledge suggests a good microservice to start with')\n",
    "        return 3, _key\n",
    "    if kscores < 45:\n",
    "        print('Little knoweldge, start to learn knowledge to expore the solution')\n",
    "        return 4, _key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db82d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_inputsemantic_match(input_file_path,namespace):\n",
    "    ft = input_file_path.split('.')[-1]\n",
    "    fn = input_file_path.split('.')[0]\n",
    "    ctxl = []\n",
    "    scorex = []\n",
    "    lags = 0 \n",
    "    if ft == 'csv':\n",
    "        inputdf = pd.read_csv(input_file_path)\n",
    "        g = Graph()\n",
    "        g.parse(\"KGLayer/contextkg.n3\")\n",
    "        q = \"\"\"\n",
    "            PREFIX ns1: <\"\"\"+namespace+\"\"\"/>\n",
    "            SELECT ?ctx ?fx\n",
    "            WHERE {?t ns1:context ?ctx . ?ctx ns1:input_data ?in . ?in ns1:data_type ns1:\"\"\"+ft+\"\"\" . ?in ns1:feature_type ?fx.}\"\"\"\n",
    "        qr = g.query(q)\n",
    "        g.close()\n",
    "        if len(qr)==0:\n",
    "            print('No input context has been find')\n",
    "        else:\n",
    "            countm = 0\n",
    "            for r in qr:\n",
    "                inpx = r[\"ctx\"]\n",
    "                #print(inpx)\n",
    "                if len(ctxl)> 0:\n",
    "                    if inpx != ctxl[-1]:\n",
    "                        countm = 0\n",
    "                        scorex.append([ctxl[-1],(lags/len(inputdf.columns))*50+10 ])\n",
    "                        lags = 0\n",
    "                        ctxl.append(inpx)\n",
    "                else:\n",
    "                    ctxl.append(inpx)    \n",
    "                inp = r[\"fx\"].split('/')[-1]\n",
    "                for col in inputdf.columns:\n",
    "                    if col == inp:\n",
    "                        countm=countm+1\n",
    "                if countm == len(inputdf.columns):\n",
    "                    scorex.append([inpx,50+10])\n",
    "                else:\n",
    "                    lags = countm\n",
    "        \n",
    "        return scorex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e00aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_outputemantic_match(output,namespace):\n",
    "    #ctxl=[]\n",
    "    scorex = []\n",
    "    g = Graph()\n",
    "    g.parse(\"KGLayer/contextkg.n3\")\n",
    "    q = \"\"\"\n",
    "        PREFIX ns1: <\"\"\"+namespace+\"\"\"/>\n",
    "        SELECT DISTINCT ?ctx\n",
    "        WHERE {?t ns1:context ?ctx . ?ctx ns1:desire_output ?up .\"\"\"\n",
    "    for xc in output:\n",
    "        outdata = xc.split('.')\n",
    "        if len(outdata)>1:\n",
    "            q=q+\"\"\"?up ns1:iocategory ns1:\"\"\"+outdata[0]+\"\"\" . \"\"\" \n",
    "            q=q+\"\"\"?up ns1:iodatatype ns1:\"\"\"+outdata[1]+\"\"\" .\"\"\"\n",
    "        else:\n",
    "            q=q+\"\"\"{?up ns1:iocategory ns1:\"\"\"+xc+\"\"\" .} UNION {?up ns1:iodatatype ns1:\"\"\"+xc+\"\"\" .}}\"\"\"\n",
    "    #print(q)\n",
    "    qr = g.query(q)\n",
    "    g.close()\n",
    "    if len(qr)==0:\n",
    "        print('No output context has been find')\n",
    "    else:\n",
    "        #countm = 0\n",
    "        for r in qr:\n",
    "            outpx = r[\"ctx\"]\n",
    "            #if len(ctxl)> 0:\n",
    "                #if outpx != ctxl[-1]:\n",
    "                    #ctxl.append(outpx)\n",
    "            #else:\n",
    "                #ctxl.append(outpx)\n",
    "            scorex.append([outpx,20])\n",
    "    return scorex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bd6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_domain_match(task_domain,namespace):\n",
    "    scorex=[]\n",
    "    g = Graph()\n",
    "    g.parse(\"KGLayer/contextkg.n3\")\n",
    "    q = \"\"\"\n",
    "        PREFIX ns1: <\"\"\"+namespace+\"\"\"/>\n",
    "        SELECT DISTINCT ?ctx\n",
    "        WHERE {?t ns1:context ?ctx . ?ctx ns1:domain ns1:\"\"\"+task_domain+\"\"\" .}\"\"\"\n",
    "    qr = g.query(q)\n",
    "    g.close()\n",
    "    if len(qr)==0:\n",
    "        print('domain is not matched')\n",
    "    else:\n",
    "        for r in qr:\n",
    "            dmx = r[\"ctx\"]\n",
    "            scorex.append([dmx,10])\n",
    "    return scorex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcfee311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_purpose_match(purpose,namespace):\n",
    "    scorex=[]\n",
    "    g = Graph()\n",
    "    g.parse(\"KGLayer/contextkg.n3\")\n",
    "    q = \"\"\"\n",
    "        PREFIX ns1: <\"\"\"+namespace+\"\"\"/>\n",
    "        SELECT DISTINCT ?ctx\n",
    "        WHERE {?t ns1:context ?ctx . ?ctx ns1:purpose ns1:\"\"\"+purpose+\"\"\" .}\"\"\"\n",
    "    qr = g.query(q)\n",
    "    g.close()\n",
    "    if len(qr)==0:\n",
    "        print('purpose is not matched')\n",
    "    else:\n",
    "        for r in qr:\n",
    "            dmx = r[\"ctx\"]\n",
    "            scorex.append([dmx,10])\n",
    "    return scorex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12aaef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_knowledge():\n",
    "    open('KGLayer/contextkg.n3', 'w').close()\n",
    "    open('KGLayer/policy.n3', 'w').close()\n",
    "    open('KGLayer/commonkg.n3', 'w').close()\n",
    "    open('KGLayer/workflows.n3', 'w').close()\n",
    "    mydir = 'KGLayer/models/'\n",
    "    filelist = [ f for f in os.listdir(mydir) if f.endswith(\".gz\") ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45df28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_knowledge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff25481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_memery={}\n",
    "servicelist=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fddd601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b9031dc0-713a-42fd-94c3-0ff311c86629 task_input KG is generated\n",
      "############Composition start##############\n",
      "No microservice has been find\n",
      "------------In while-----------\n",
      "Successfully imported loadnormalpddata microservice.\n",
      "No microservice has been find\n",
      "...\n",
      "------------In while-----------\n",
      "Successfully imported spliting microservice.\n",
      "find_match_paramenter---\n",
      "find_match_paramenter---\n",
      "------------In while-----------\n",
      "find_match_paramenter---\n",
      "find_match_paramenter---\n",
      "find_match_paramenter---\n",
      "Successfully imported pipelinemodels microservice.\n",
      "find_match_paramenter---\n",
      "There is no matched the service can take the last output as input\n",
      "Agent is working on combinations of all previous outputs to search a possible solution\n",
      "...continue searching solution...\n",
      "wow! find combination output -> input\n",
      "Successfully imported featuremodelevaluation microservice.\n",
      "-------end while-------\n",
      "############Composition sucessfully complete##############\n"
     ]
    }
   ],
   "source": [
    "value = OperatingTask ('task1','heart.csv',['pipeline'],'medical','','MLmodel_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8178252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[rdflib.term.URIRef('http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f'),\n",
       "  60]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_inputsemantic_match('heart.csv','http://aimicroservice.derby.ac.uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be873304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[rdflib.term.URIRef('http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f'),\n",
       "  20]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_outputemantic_match(['pipeline'],'http://aimicroservice.derby.ac.uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a56d96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[rdflib.term.URIRef('http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f'),\n",
       "  10]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_domain_match('medical','http://aimicroservice.derby.ac.uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db378323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[rdflib.term.URIRef('http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f'),\n",
       "  10]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_purpose_match('MLmodel_classification','http://aimicroservice.derby.ac.uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77ffa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for solution ...\n",
      "{rdflib.term.URIRef('http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f'): 100}\n",
      "from the policy knowledge, solution found http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdflib.term.URIRef('http://aimicroservice.derby.ac.uk/73e32331-32bc-434e-9e26-0fdcb44c7c6f')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_reasoning ('heart.csv',['pipeline'],'medical','http://aimicroservice.derby.ac.uk','MLmodel_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d06c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loadnormalpddata', 'spliting', 'pipelinemodels', 'featuremodelevaluation']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "servicelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83b0700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='median')),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000022641F1B5E0>),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000022641F1B640>)])),\n",
       "                ('rfe',\n",
       "                 RFE(estimator=LogisticRegression(), n_features_to_select=8))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e290a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasetout=value[2]['datafile.pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2c42d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def outputlearning(datasetout,task_id)\n",
    " #   select_pipem=value[0]\n",
    "  #  select_pipem.fit(datasetout.drop(datasetout.columns[-1], axis=1),datasetout[datasetout.columns[-1]].astype(int))\n",
    "   # print(select_pipem.named_steps['rfe'].support_,select_pipem.named_steps['rfe'].ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82f0bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonKGlearning(datasetout,task_id,outputv,ns):\n",
    "    namespace = ns\n",
    "    select_pipem=outputv\n",
    "    select_pipem.fit(datasetout.drop(datasetout.columns[-1], axis=1),datasetout[datasetout.columns[-1]].astype(int))\n",
    "    k=select_pipem.named_steps['rfe'].support_,select_pipem.named_steps['rfe'].ranking_\n",
    "    listn=''\n",
    "    i=0\n",
    "    flag=0\n",
    "    for ik in k[1]:\n",
    "        if ik == 1:\n",
    "            if flag ==0:\n",
    "                listn=str(i)\n",
    "                flag = 1\n",
    "            else:\n",
    "                listn=listn+'.'+str(i)\n",
    "        i=i+1\n",
    "    kg = Graph()\n",
    "    kg.parse(\"KGLayer/taskinput.n3\")\n",
    "    #subject\n",
    "    task = URIRef(namespace+'/'+task_id)\n",
    "    #verb\n",
    "    has_select_f = URIRef(namespace+'/selectedfeatures')\n",
    "    kg.add((task, has_select_f, Literal(listn,lang=\"en\")))\n",
    "    #print(kg)\n",
    "    kg.serialize(destination='KGLayer/taskinput'+\".n3\")\n",
    "    kg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2ca44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlearning(datasetout,'912e385e-2b93-470a-9cb5-2f93cb00783d',value[0], 'http://aimicroservice.derby.ac.uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ecdf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datafile.pandas':      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       " 0     63    1   3       145   233    1        0      150      0      2.3   \n",
       " 1     37    1   2       130   250    0        1      187      0      3.5   \n",
       " 2     41    0   1       130   204    0        0      172      0      1.4   \n",
       " 3     56    1   1       120   236    0        1      178      0      0.8   \n",
       " 4     57    0   0       120   354    0        1      163      1      0.6   \n",
       " ..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       " 298   57    0   0       140   241    0        1      123      1      0.2   \n",
       " 299   45    1   3       110   264    0        1      132      0      1.2   \n",
       " 300   68    1   0       144   193    1        1      141      0      3.4   \n",
       " 301   57    1   0       130   131    0        1      115      1      1.2   \n",
       " 302   57    0   1       130   236    0        0      174      0      0.0   \n",
       " \n",
       "      slope  ca  thal  target  \n",
       " 0        0   0     1       1  \n",
       " 1        0   0     2       1  \n",
       " 2        2   0     2       1  \n",
       " 3        2   0     2       1  \n",
       " 4        2   0     2       1  \n",
       " ..     ...  ..   ...     ...  \n",
       " 298      1   0     3       0  \n",
       " 299      1   0     3       0  \n",
       " 300      1   2     3       0  \n",
       " 301      1   1     3       0  \n",
       " 302      1   1     2       0  \n",
       " \n",
       " [303 rows x 14 columns],\n",
       " 'datafile.split.#X_train, X_test, y_train, y_test#': [{'2': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=2))]),\n",
       "   '3': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=3))]),\n",
       "   '4': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=4))]),\n",
       "   '5': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=5))]),\n",
       "   '6': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=6))]),\n",
       "   '7': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=7))]),\n",
       "   '8': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=8))]),\n",
       "   '9': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=9))]),\n",
       "   '10': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=10))]),\n",
       "   '11': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=11))]),\n",
       "   '12': Pipeline(steps=[('columntransformer',\n",
       "                    ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(add_indicator=True,\n",
       "                                                                                    strategy='median')),\n",
       "                                                                     ('standardscaler',\n",
       "                                                                      StandardScaler())]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB4580>),\n",
       "                                                    ('pipeline-2',\n",
       "                                                     Pipeline(steps=[('simpleimputer',\n",
       "                                                                      SimpleImputer(strategy='constant')),\n",
       "                                                                     ('onehotencoder',\n",
       "                                                                      OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                     <sklearn.compose._column_transformer.make_column_selector object at 0x0000016463EB45E0>)])),\n",
       "                   ('rfe',\n",
       "                    RFE(estimator=LogisticRegression(), n_features_to_select=12))])}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d34203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        PREFIX ns1: <http://aimicroservice.derby.ac.uk/>\n",
      "        \n",
      "        SELECT *\n",
      "        WHERE {?ms rdf:type ns1:ETL_load .\n",
      "     ?ms ns1:input ?in . ?in ns1:paramter ?ip .?ip ns1:iocategory ns1:datafile . ?ip ns1:iodatatype ns1:csv . ?ms ns1:output ?out . ?out ns1:paramter ?up .?up ns1:iocategory ns1:datafile . ?up ns1:iodatatype ns1:pandas .}\n",
      "loadnormalpddata\n",
      "Successfully imported loadnormalpddata microservice.\n",
      "input:  heart.csv ->knowledge:  ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n"
     ]
    }
   ],
   "source": [
    "output_value = sandi.searchandinvoke ('ETL_load',['datafile.csv'], ['datafile.pandas'], ['heart.csv'], 'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551803fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loadnormalpddata'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17d1bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalling\n",
      "Successfully imported scalling microservice.\n"
     ]
    }
   ],
   "source": [
    "output_value1 = sandi.searchandinvoke ('ETL_transform','', ['pipeline.processor'], '','task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72096ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliting\n",
      "Successfully imported spliting microservice.\n"
     ]
    }
   ],
   "source": [
    "output_value2 = sandi.searchandinvoke ('DE_split',['datafile.pandas','varable.str'], ['datafile.pandas'], [output_value,'target'],'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f3efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrsklmodel\n",
      "Successfully imported lrsklmodel microservice.\n"
     ]
    }
   ],
   "source": [
    "output_value3 = sandi.searchandinvoke ('MLmodel_classification','', ['model.logistic_regression'], '','task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f686d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureengineering\n",
      "Successfully imported featureengineering microservice.\n"
     ]
    }
   ],
   "source": [
    "output_value4 = sandi.searchandinvoke ('ML_featuretest',['datafile.logistic_regression','model.pandas','pipeline.processor'], ['model.dict'], [output_value2[0],output_value2[1],output_value3,output_value1],'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fba9b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuremodelevaluation\n",
      "Successfully imported featuremodelevaluation microservice.\n"
     ]
    }
   ],
   "source": [
    "output_value5 = sandi.searchandinvoke ('ML_optimisation',['datafile.pandas','model.dict'], ['score.float','model.dict'], [output_value4,output_value2[3],output_value2[5]],'task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a07cbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='median')),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000023783CC3520>),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000023783CC36A0>)])),\n",
       "                ('rfe',\n",
       "                 RFE(estimator=LogisticRegression(), n_features_to_select=4))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_value5[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f1bc01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False False False False False  True False  True\n",
      " False] [ 9  1  1  6  8 10  7  2  4  1  5  1  3]\n"
     ]
    }
   ],
   "source": [
    "select_pipem=output_value5[1]\n",
    "select_pipem.fit(output_value2[0],output_value2[1])\n",
    "print(select_pipem.named_steps['rfe'].support_,select_pipem.named_steps['rfe'].ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2fda51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sex'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_value.columns.values.tolist()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f956ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_output_workflowKG (listOfms,task_id,ns):\n",
    "    namespace=ns\n",
    "    if len(listOfms)>0:\n",
    "        kg = Graph()\n",
    "        kg.parse(\"KGLayer/workflows.n3\")\n",
    "        \n",
    "        if len(task_id):\n",
    "            #subject\n",
    "            task = URIRef(namespace+'/'+task_id)\n",
    "            #verb\n",
    "            has_workflow = URIRef(namespace+'/workflow')\n",
    "            has_ms_id = URIRef(namespace+'/wf_id')\n",
    "            has_ims = URIRef(namespace+'/wf_ims')\n",
    "            has_ms_iloc = URIRef(namespace+'/wf_iloc')\n",
    "            _ms = BNode()\n",
    "            kg.add((task, has_workflow, _ms))\n",
    "            i=0\n",
    "            #object\n",
    "            for ms in listOfms:\n",
    "                _ims = BNode()\n",
    "                kg.add((_ms, has_ims, _ims))\n",
    "                kg.add((_ims, has_ms_id, Literal(str(i),lang=\"en\")))\n",
    "                kg.add((_ims, has_ms_iloc, Literal(ms,lang=\"en\")))\n",
    "                i=i+1\n",
    "        kg.serialize(destination='KGLayer/workflows'+\".n3\")\n",
    "        kg.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d69d404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9dd4807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savemodel(task_id,model,ns):\n",
    "    namespace=ns\n",
    "    filename = 'KGLayer/models/'+task_id\n",
    "    with gzip.GzipFile(filename + '.gz', 'wb', compresslevel=3) as fo:  \n",
    "        joblib.dump(model, fo)\n",
    "    kg = Graph()\n",
    "    kg.parse(\"KGLayer/solution.n3\")\n",
    "    #subject\n",
    "    task = URIRef(namespace+'/'+task_id)\n",
    "    #verb\n",
    "    has_model = URIRef(namespace+'/solution')\n",
    "    kg.add((task, has_model, Literal(filename+'.gz',lang=\"en\")))\n",
    "    #print(kg)\n",
    "    kg.serialize(destination='KGLayer/solution'+\".n3\")\n",
    "    kg.close()\n",
    "    #with gzip.GzipFile(filename + '.gz', 'rb') as fo:  \n",
    "        #return joblib.load(fo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e18f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
